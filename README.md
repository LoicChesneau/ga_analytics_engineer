# Getaround EU Analytics Engineering Challenge (previously Drivy)

Looking for a job? Check out our [open positions](https://www.welcometothejungle.com/en/companies/getaround/jobs).
You can also take a look at our [engineering blog](https://drivy.engineering/) to learn more about the way we work.

## Guidelines

- Clone this repo (do **not** fork it)
- Solve the exercises in  ascending order
- Only do one commit per exercise, include the `.git` when submiting your work

Please do the simplest thing that could work for the exercise you're currently solving.

For higher levels we are interested in seeing code that is:

- Clean
- Extensible
- Reliable

## Challenge

The challenge needs to be resolved in PostgreSQL.
Exercises depend on datasets available in /data/
**You can't modify them.**

Your solution to exercise {N} needs to live in the `worksheets_{N}` directory as .sql files.

For certain parts of the challenge, you will be asked to provide written answers, in these cases, please submit them in `worksheets_{N}` as markdown files.

## Setup

### 0.1

Create a local Postgresql instance, ideally 13.3 on MacOS

### 0.2

Create a table named `raw_payloads` in the SQL instance you have just created and insert the payloads provided in `./data/tracking_payloads_20210603.csv`.

### 0.3

Create another table named `events` and insert a parsed version of `raw_payloads` inside of it.

## Exercise 1

The payload you have been provided with are generated whenever a user triggers an action relevant to our analytics on the platform. 

Each event you have parsed out of these raw payloads respects a default structure:
- `timestamp`: generated at the exact time of the action.
- `anonymous_id`: associated to a user device
- `action_id`: associated to the action

With an optional additional key:
- `url`: url the user has triggered the action from

We want to aggregate these events together to analyse user behavior over a certain period of time.

### 1.1 

Create a table that aggregates events per session of events. If a user has stopped their activity for more than 30 minutes, we should consider the session as ended.

The SQL query you design to create this table should ultimately respect the following format:
- `anonymous_id`
- `session_id`
- `session_start`
- `session_end`

Explain your design in your markdown file.

### 1.2 

We also want to know which marketing action has attracted the user to begin their session. Thanksfully, `url` provide us with `utm_source`, `medium` and `campaign` details we can use to find out. For more details about UTM, check out this [blog post](https://buffer.com/library/utm-guide/)

Enrich your `session` table with a `source` column that gives your detail about the marketing action associated to each `session_id`.


## Exercise 2

Another set of raw payloads is provided to you in `./data/tracking_payloads_20210604.csv`.

### 2.1

Write the serie of queries that will succesively update `raw_payloads`, `payloads` and `sessions` in your SQL file.

Explain your approach in the markdown file.

### 2.2

Do you observe issues in your computations because of this additional file? If so, create a second SQL file to show us how you would adapt your transformation queries to produce reliable tables for analytics.

Detail the issues you have noticed and explain how your design fixes them in your markdown file.

## Exercise 3

In reality, millions of payloads are generated by platforms like ours everyday.

Without any additional dataset, find a way to challenge your query with a larger volume of data (> 1000 rows). Write your workaround methods in a SQL file.

Explain your approach in the markdown file.

## Exercise 4

Write queries that you could run to test your raw and transformed tables.

In your markdown file, tell us the way these queries should be run to optimally test your data.
